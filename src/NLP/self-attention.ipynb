{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T05:43:09.118863Z",
     "start_time": "2019-08-20T05:42:56.780937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chan 2019-08-20 \n",
      "\n",
      "numpy 1.16.4\n",
      "pandas 0.24.2\n",
      "konlpy 0.5.1\n",
      "torch 1.0.1\n",
      "keras 2.2.4\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a Chan -d -p numpy,pandas,konlpy,torch,keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T05:50:40.268127Z",
     "start_time": "2019-08-20T05:50:40.203121Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import konlpy\n",
    "from utils import morp_preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T05:44:49.590260Z",
     "start_time": "2019-08-20T05:44:49.446341Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment</th>\n",
       "      <th>url_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ㅜㅜ</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>헐</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>제시</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>이거인 듯</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                comment  url_id  label\n",
       "0           0                     ㅜㅜ      77      0\n",
       "1           1  ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ      17      0\n",
       "2           2                      헐      52      0\n",
       "3           3                     제시      75      0\n",
       "4           4                  이거인 듯      18      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = pd.read_csv('../../data/train.csv', encoding='utf-16')\n",
    "datasets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T05:52:27.236751Z",
     "start_time": "2019-08-20T05:51:05.784453Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skarn\\Anaconda3\\envs\\DataAnalysis\\lib\\site-packages\\jpype\\_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 42000/42000 [01:15<00:00, 559.36it/s]\n"
     ]
    }
   ],
   "source": [
    "processed = morp_preprocessing.chat_to_morp(datasets.comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T06:16:28.123959Z",
     "start_time": "2019-08-20T06:16:28.114961Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T07:16:31.525423Z",
     "start_time": "2019-08-20T07:16:31.490442Z"
    }
   },
   "outputs": [],
   "source": [
    "#Please create your own dataloader for new datasets of the following type\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import torch.utils.data as data_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    " \n",
    "def load_data_set(max_len,vocab_size,batch_size):\n",
    "    \"\"\"\n",
    "        Loads the dataset. Keras Imdb dataset for binary classifcation. Keras reuters dataset for multiclass classification\n",
    " \n",
    "        Args:\n",
    "            type   : {bool} 0 for binary classification returns imdb dataset. 1 for multiclass classfication return reuters set\n",
    "            max_len: {int} timesteps used for padding\n",
    "\t\t\tvocab_size: {int} size of the vocabulary\n",
    "\t\t\tbatch_size: batch_size\n",
    "        Returns:\n",
    "            train_loader: {torch.Dataloader} train dataloader\n",
    "            x_test_pad  : padded tokenized test_data for cross validating\n",
    "\t\t\ty_test      : y_test\n",
    "            word_to_id  : {dict} words mapped to indices\n",
    " \n",
    "      \n",
    "        \"\"\"\n",
    "   \n",
    "    INDEX_FROM=2\n",
    "    \n",
    "    NUM_WORDS=vocab_size # only use top 1000 words\n",
    "       # word index offset\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(processed, np.asarray(datasets.label))\n",
    "    \n",
    "\n",
    "    with open('vocab/vocab_index.pickle', 'rb') as f:\n",
    "        vocab_index = pickle.load(f)\n",
    "\n",
    "    word_to_id =  vocab_index\n",
    "    word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "#         word_to_id[\"<PAD>\"] = 0\n",
    "#         word_to_id[\"<START>\"] = 1\n",
    "#         word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "    id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "#     x = np.concatenate([x_train, x_test])\n",
    "#     y = np.concatenate([y_train, y_test])\n",
    "#     n_train = x.shape[0] - 1000\n",
    "#     n_valid = 1000\n",
    "\n",
    "#     x_train = x[:n_train]\n",
    "#     y_train = y[:n_train]\n",
    "#     x_test = x[n_train:n_train+n_valid]\n",
    "#     y_test = y[n_train:n_train+n_valid]\n",
    "\n",
    "\n",
    "    #embeddings = load_glove_embeddings(\"../../GloVe/glove.6B.50d.txt\",word_to_id,50)\n",
    "    x_train_pad = pad_sequences(x_train,maxlen=max_len)\n",
    "    x_test_pad = pad_sequences(x_test,maxlen=max_len)\n",
    "\n",
    "\n",
    "    train_data = data_utils.TensorDataset(torch.from_numpy(x_train_pad).type(torch.LongTensor),torch.from_numpy(y_train).type(torch.DoubleTensor))\n",
    "    train_loader = data_utils.DataLoader(train_data,batch_size=batch_size,drop_last=True)\n",
    "    return train_loader,x_test_pad,y_test,word_to_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T06:24:23.346431Z",
     "start_time": "2019-08-20T06:24:23.191519Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch,keras\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    " \n",
    "class StructuredSelfAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The class is an implementation of the paper A Structured Self-Attentive Sentence Embedding including regularization\n",
    "    and without pruning. Slight modifications have been done for speedup\n",
    "    \"\"\"\n",
    "   \n",
    "    def __init__(self,batch_size,lstm_hid_dim,d_a,r,max_len,emb_dim=100,vocab_size=None,use_pretrained_embeddings = False,embeddings=None,type=0,n_classes = 1):\n",
    "        \"\"\"\n",
    "        Initializes parameters suggested in paper\n",
    " \n",
    "        Args:\n",
    "            batch_size  : {int} batch_size used for training\n",
    "            lstm_hid_dim: {int} hidden dimension for lstm\n",
    "            d_a         : {int} hidden dimension for the dense layer\n",
    "            r           : {int} attention-hops or attention heads\n",
    "            max_len     : {int} number of lstm timesteps\n",
    "            emb_dim     : {int} embeddings dimension\n",
    "            vocab_size  : {int} size of the vocabulary\n",
    "            use_pretrained_embeddings: {bool} use or train your own embeddings\n",
    "            embeddings  : {torch.FloatTensor} loaded pretrained embeddings\n",
    "            type        : [0,1] 0-->binary_classification 1-->multiclass classification\n",
    "            n_classes   : {int} number of classes\n",
    " \n",
    "        Returns:\n",
    "            self\n",
    " \n",
    "        Raises:\n",
    "            Exception\n",
    "        \"\"\"\n",
    "        super(StructuredSelfAttention,self).__init__()\n",
    "       \n",
    "        self.embeddings,emb_dim = self._load_embeddings(use_pretrained_embeddings,embeddings,vocab_size,emb_dim)\n",
    "        self.lstm = torch.nn.LSTM(emb_dim,lstm_hid_dim,1,batch_first=True)\n",
    "        self.linear_first = torch.nn.Linear(lstm_hid_dim,d_a)\n",
    "        self.linear_first.bias.data.fill_(0)\n",
    "        self.linear_second = torch.nn.Linear(d_a,r)\n",
    "        self.linear_second.bias.data.fill_(0)\n",
    "        self.n_classes = n_classes\n",
    "        self.linear_final = torch.nn.Linear(lstm_hid_dim,self.n_classes)\n",
    "        self.batch_size = batch_size       \n",
    "        self.max_len = max_len\n",
    "        self.lstm_hid_dim = lstm_hid_dim\n",
    "        self.hidden_state = self.init_hidden()\n",
    "        self.r = r\n",
    "        self.type = type\n",
    "                 \n",
    "    def _load_embeddings(self,use_pretrained_embeddings,embeddings,vocab_size,emb_dim):\n",
    "        \"\"\"Load the embeddings based on flag\"\"\"\n",
    "       \n",
    "        if use_pretrained_embeddings is True and embeddings is None:\n",
    "            raise Exception(\"Send a pretrained word embedding as an argument\")\n",
    "           \n",
    "        if not use_pretrained_embeddings and vocab_size is None:\n",
    "            raise Exception(\"Vocab size cannot be empty\")\n",
    "   \n",
    "        if not use_pretrained_embeddings:\n",
    "            word_embeddings = torch.nn.Embedding(vocab_size,emb_dim,padding_idx=0)\n",
    "            \n",
    "        elif use_pretrained_embeddings:\n",
    "            word_embeddings = torch.nn.Embedding(embeddings.size(0), embeddings.size(1))\n",
    "            word_embeddings.weight = torch.nn.Parameter(embeddings)\n",
    "            emb_dim = embeddings.size(1)\n",
    "            \n",
    "        return word_embeddings,emb_dim\n",
    "       \n",
    "        \n",
    "    def softmax(self,input, axis=1):\n",
    "        \"\"\"\n",
    "        Softmax applied to axis=n\n",
    " \n",
    "        Args:\n",
    "           input: {Tensor,Variable} input on which softmax is to be applied\n",
    "           axis : {int} axis on which softmax is to be applied\n",
    " \n",
    "        Returns:\n",
    "            softmaxed tensors\n",
    " \n",
    "       \n",
    "        \"\"\"\n",
    " \n",
    "        input_size = input.size()\n",
    "        trans_input = input.transpose(axis, len(input_size)-1)\n",
    "        trans_size = trans_input.size()\n",
    "        input_2d = trans_input.contiguous().view(-1, trans_size[-1])\n",
    "        soft_max_2d = F.softmax(input_2d)\n",
    "        soft_max_nd = soft_max_2d.view(*trans_size)\n",
    "        return soft_max_nd.transpose(axis, len(input_size)-1)\n",
    "       \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (Variable(torch.zeros(1,self.batch_size,self.lstm_hid_dim)),Variable(torch.zeros(1,self.batch_size,self.lstm_hid_dim)))\n",
    "       \n",
    "        \n",
    "    def forward(self,x):\n",
    "        embeddings = self.embeddings(x)       \n",
    "        outputs, self.hidden_state = self.lstm(embeddings.view(self.batch_size,self.max_len,-1),self.hidden_state)       \n",
    "        x = F.tanh(self.linear_first(outputs))       \n",
    "        x = self.linear_second(x)       \n",
    "        x = self.softmax(x,1)       \n",
    "        attention = x.transpose(1,2)       \n",
    "        sentence_embeddings = attention@outputs       \n",
    "        avg_sentence_embeddings = torch.sum(sentence_embeddings,1)/self.r\n",
    "       \n",
    "        if not bool(self.type):\n",
    "            output = F.sigmoid(self.linear_final(avg_sentence_embeddings))\n",
    "           \n",
    "            return output,attention\n",
    "        else:\n",
    "            return F.log_softmax(self.linear_final(avg_sentence_embeddings)),attention\n",
    "       \n",
    "\n",
    "    #Regularization\n",
    "    def l2_matrix_norm(self,m):\n",
    "        \"\"\"\n",
    "        Frobenius norm calculation\n",
    " \n",
    "        Args:\n",
    "           m: {Variable} ||AAT - I||\n",
    " \n",
    "        Returns:\n",
    "            regularized value\n",
    " \n",
    "       \n",
    "        \"\"\"\n",
    "        return torch.sum(torch.sum(torch.sum(m**2,1),1)**0.5).type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T08:07:53.783095Z",
     "start_time": "2019-08-20T08:07:53.100469Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader, x_test_pad, y_test, word_to_id = load_data_set(200, 30000, 512)  # loading imdb dataset\n",
    "attention_model = StructuredSelfAttention(batch_size=train_loader.batch_size,\n",
    "                                              lstm_hid_dim=50,\n",
    "                                              d_a=100,\n",
    "                                              r=10,\n",
    "                                              vocab_size=30000, \n",
    "                                              max_len=200, \n",
    "                                              type=0, \n",
    "                                              n_classes=1,\n",
    "                                              use_pretrained_embeddings=False,\n",
    "                                              embeddings=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T06:25:47.828315Z",
     "start_time": "2019-08-20T06:25:47.754359Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    " \n",
    "def train(attention_model,train_loader,criterion,optimizer,epochs = 5,use_regularization = False,C=0,clip=False):\n",
    "    \"\"\"\n",
    "        Training code\n",
    " \n",
    "        Args:\n",
    "            attention_model : {object} model\n",
    "            train_loader    : {DataLoader} training data loaded into a dataloader\n",
    "            optimizer       :  optimizer\n",
    "            criterion       :  loss function. Must be BCELoss for binary_classification and NLLLoss for multiclass\n",
    "            epochs          : {int} number of epochs\n",
    "            use_regularizer : {bool} use penalization or not\n",
    "            C               : {int} penalization coeff\n",
    "            clip            : {bool} use gradient clipping or not\n",
    "       \n",
    "        Returns:\n",
    "            accuracy and losses of the model\n",
    " \n",
    "      \n",
    "        \"\"\"\n",
    "    losses = []\n",
    "    accuracy = []\n",
    "    for i in range(epochs):\n",
    "        print(\"Running EPOCH\",i+1)\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "        correct = 0\n",
    "       \n",
    "        for batch_idx,train in enumerate(train_loader):\n",
    " \n",
    "            attention_model.hidden_state = attention_model.init_hidden()\n",
    "            x,y = Variable(train[0]),Variable(train[1])\n",
    "            y_pred,att = attention_model(x)\n",
    "           \n",
    "            #penalization AAT - I\n",
    "            if use_regularization:\n",
    "                attT = att.transpose(1,2)\n",
    "                identity = torch.eye(att.size(1))\n",
    "                identity = Variable(identity.unsqueeze(0).expand(train_loader.batch_size,att.size(1),att.size(1)))\n",
    "                penal = attention_model.l2_matrix_norm(att@attT - identity)\n",
    "           \n",
    "            \n",
    "            if not bool(attention_model.type) :\n",
    "                #binary classification\n",
    "                #Adding a very small value to prevent BCELoss from outputting NaN's\n",
    "                correct+=torch.eq(torch.round(y_pred.type(torch.DoubleTensor).squeeze(1)),y).data.sum()\n",
    "                if use_regularization:\n",
    "                    try:\n",
    "                        loss = criterion(y_pred.type(torch.DoubleTensor).squeeze(1)+1e-8,y) + C * penal/train_loader.batch_size\n",
    "                       \n",
    "                    except RuntimeError:\n",
    "                        raise Exception(\"BCELoss gets nan values on regularization. Either remove regularization or add very small values\")\n",
    "                else:\n",
    "                    loss = criterion(y_pred.type(torch.DoubleTensor).squeeze(1),y)\n",
    "                \n",
    "            \n",
    "            else:\n",
    "                \n",
    "                correct+=torch.eq(torch.max(y_pred,1)[1],y.type(torch.LongTensor)).data.sum()\n",
    "                if use_regularization:\n",
    "                    loss = criterion(y_pred,y) + (C * penal/train_loader.batch_size).type(torch.FloatTensor)\n",
    "                else:\n",
    "                    loss = criterion(y_pred,y)\n",
    "               \n",
    " \n",
    "            total_loss+=loss.data\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "           \n",
    "            #gradient clipping\n",
    "            if clip:\n",
    "                torch.nn.utils.clip_grad_norm(attention_model.parameters(),0.5)\n",
    "            optimizer.step()\n",
    "            n_batches+=1\n",
    "           \n",
    "        print(\"avg_loss is\",total_loss/n_batches)\n",
    "        print(\"Accuracy of the model\",correct/(n_batches*train_loader.batch_size))\n",
    "        losses.append(total_loss/n_batches)\n",
    "        accuracy.append(correct/(n_batches*train_loader.batch_size))\n",
    "    return losses,accuracy\n",
    " \n",
    "def evaluate(attention_model,x_test,y_test):\n",
    "    \"\"\"\n",
    "        cv results\n",
    " \n",
    "        Args:\n",
    "            attention_model : {object} model\n",
    "            x_test          : {nplist} x_test\n",
    "            y_test          : {nplist} y_test\n",
    "       \n",
    "        Returns:\n",
    "            cv-accuracy\n",
    " \n",
    "      \n",
    "    \"\"\"\n",
    "   \n",
    "    attention_model.batch_size = x_test.shape[0]\n",
    "    attention_model.hidden_state = attention_model.init_hidden()\n",
    "    x_test_var = Variable(torch.from_numpy(x_test).type(torch.LongTensor))\n",
    "    y_test_pred,_ = attention_model(x_test_var)\n",
    "    if bool(attention_model.type):\n",
    "        y_preds = torch.max(y_test_pred,1)[1]\n",
    "        y_test_var = Variable(torch.from_numpy(y_test).type(torch.LongTensor))\n",
    "       \n",
    "    else:\n",
    "        y_preds = torch.round(y_test_pred.type(torch.DoubleTensor).squeeze(1))\n",
    "        y_test_var = Variable(torch.from_numpy(y_test).type(torch.DoubleTensor))\n",
    "       \n",
    "    return torch.eq(y_preds,y_test_var).data.sum()/x_test_var.size(0)\n",
    " \n",
    "def get_activation_wts(attention_model,x):\n",
    "    \"\"\"\n",
    "        Get r attention heads\n",
    " \n",
    "        Args:\n",
    "            attention_model : {object} model\n",
    "            x               : {torch.Variable} input whose weights we want\n",
    "       \n",
    "        Returns:\n",
    "            r different attention weights\n",
    " \n",
    "      \n",
    "    \"\"\"\n",
    "    attention_model.batch_size = x.size(0)\n",
    "    attention_model.hidden_state = attention_model.init_hidden()\n",
    "    _,wts = attention_model(x)\n",
    "    return wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T08:28:24.756302Z",
     "start_time": "2019-08-20T08:07:56.368594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skarn\\Anaconda3\\envs\\DataAnalysis\\lib\\site-packages\\torch\\nn\\functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "C:\\Users\\skarn\\Anaconda3\\envs\\DataAnalysis\\lib\\site-packages\\ipykernel_launcher.py:91: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\skarn\\Anaconda3\\envs\\DataAnalysis\\lib\\site-packages\\torch\\nn\\functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\skarn\\Anaconda3\\envs\\DataAnalysis\\lib\\site-packages\\ipykernel_launcher.py:74: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss is tensor(0.2898, dtype=torch.float64)\n",
      "Accuracy of the model tensor(0)\n",
      "Running EPOCH 2\n",
      "avg_loss is tensor(0.2387, dtype=torch.float64)\n",
      "Accuracy of the model tensor(0)\n",
      "Running EPOCH 3\n",
      "avg_loss is tensor(0.2048, dtype=torch.float64)\n",
      "Accuracy of the model tensor(0)\n",
      "Running EPOCH 4\n",
      "avg_loss is tensor(0.1801, dtype=torch.float64)\n",
      "Accuracy of the model tensor(0)\n",
      "Running EPOCH 5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at ..\\aten\\src\\TH\\THGeneral.cpp:201",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-ab450f77007a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m losses, accuracy = train(attention_model, train_loader, loss, optimizer, epochs=5,\n\u001b[1;32m----> 4\u001b[1;33m                          use_regularization=True, C=0.03, clip=True)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-c0c8670f989f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(attention_model, train_loader, criterion, optimizer, epochs, use_regularization, C, clip)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0mtotal_loss\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;31m#gradient clipping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DataAnalysis\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DataAnalysis\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at ..\\aten\\src\\TH\\THGeneral.cpp:201"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.RMSprop(attention_model.parameters())\n",
    "losses, accuracy = train(attention_model, train_loader, loss, optimizer, epochs=5,\n",
    "                         use_regularization=True, C=0.03, clip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T07:38:28.399960Z",
     "start_time": "2019-08-20T07:37:13.666811Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skarn\\Anaconda3\\envs\\DataAnalysis\\lib\\site-packages\\ipykernel_launcher.py:91: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights for the testing data in binary classification are: tensor([[[6.6596e-01, 3.3026e-01, 3.7529e-03,  ..., 3.5268e-10,\n",
      "          3.3828e-10, 2.4966e-07],\n",
      "         [2.1786e-09, 1.3291e-09, 2.9249e-09,  ..., 6.9241e-12,\n",
      "          4.8197e-12, 3.0312e-01],\n",
      "         [1.2909e-08, 1.2504e-09, 9.4795e-09,  ..., 3.5037e-05,\n",
      "          1.9510e-04, 4.9763e-06],\n",
      "         ...,\n",
      "         [1.5439e-11, 1.2078e-12, 6.0075e-12,  ..., 1.5603e-05,\n",
      "          1.5612e-06, 8.1863e-04],\n",
      "         [1.5809e-04, 4.5176e-03, 1.4398e-01,  ..., 7.2862e-15,\n",
      "          4.4046e-13, 3.7180e-13],\n",
      "         [4.8935e-08, 5.1401e-09, 4.4072e-08,  ..., 9.3209e-03,\n",
      "          9.5218e-03, 1.0529e-03]],\n",
      "\n",
      "        [[6.6596e-01, 3.3026e-01, 3.7529e-03,  ..., 9.7322e-09,\n",
      "          1.5156e-08, 1.2897e-08],\n",
      "         [8.6980e-10, 5.3064e-10, 1.1678e-09,  ..., 9.6619e-09,\n",
      "          3.5849e-08, 1.7947e-08],\n",
      "         [1.3004e-08, 1.2595e-09, 9.5490e-09,  ..., 2.3950e-07,\n",
      "          9.9275e-08, 8.4737e-08],\n",
      "         ...,\n",
      "         [1.5249e-11, 1.1930e-12, 5.9335e-12,  ..., 1.2109e-06,\n",
      "          7.0801e-07, 4.6173e-07],\n",
      "         [1.5809e-04, 4.5176e-03, 1.4398e-01,  ..., 9.8398e-18,\n",
      "          4.1975e-18, 3.0815e-18],\n",
      "         [6.6032e-08, 6.9360e-09, 5.9471e-08,  ..., 6.4402e-04,\n",
      "          3.8319e-04, 3.5103e-04]],\n",
      "\n",
      "        [[6.6596e-01, 3.3026e-01, 3.7529e-03,  ..., 1.6432e-08,\n",
      "          1.2866e-08, 1.4493e-08],\n",
      "         [9.1247e-10, 5.5667e-10, 1.2251e-09,  ..., 4.4799e-08,\n",
      "          1.7149e-08, 2.5583e-08],\n",
      "         [1.3013e-08, 1.2604e-09, 9.5554e-09,  ..., 5.9849e-08,\n",
      "          6.7915e-08, 6.2176e-08],\n",
      "         ...,\n",
      "         [1.5034e-11, 1.1761e-12, 5.8496e-12,  ..., 4.3874e-07,\n",
      "          3.4707e-07, 3.6444e-07],\n",
      "         [1.5809e-04, 4.5176e-03, 1.4398e-01,  ..., 2.5190e-18,\n",
      "          2.3903e-18, 2.2499e-18],\n",
      "         [6.8155e-08, 7.1590e-09, 6.1382e-08,  ..., 2.9903e-04,\n",
      "          3.2066e-04, 3.0561e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[6.6596e-01, 3.3025e-01, 3.7529e-03,  ..., 4.9662e-07,\n",
      "          3.7107e-07, 2.4640e-07],\n",
      "         [1.5235e-07, 9.2942e-08, 2.0454e-07,  ..., 1.0173e-01,\n",
      "          1.2865e-01, 8.5111e-02],\n",
      "         [2.4714e-09, 2.3938e-10, 1.8148e-09,  ..., 8.9468e-02,\n",
      "          5.0909e-02, 2.9679e-02],\n",
      "         ...,\n",
      "         [3.2662e-13, 2.5551e-14, 1.2709e-13,  ..., 9.8688e-02,\n",
      "          7.9850e-02, 5.6066e-02],\n",
      "         [1.5809e-04, 4.5176e-03, 1.4398e-01,  ..., 1.2241e-08,\n",
      "          8.5305e-09, 4.1898e-09],\n",
      "         [3.1259e-09, 3.2834e-10, 2.8153e-09,  ..., 1.0039e-01,\n",
      "          6.9471e-02, 5.0459e-02]],\n",
      "\n",
      "        [[6.6596e-01, 3.3026e-01, 3.7529e-03,  ..., 1.2688e-08,\n",
      "          1.4492e-08, 1.3812e-08],\n",
      "         [1.8319e-09, 1.1176e-09, 2.4595e-09,  ..., 3.3703e-08,\n",
      "          5.2153e-08, 4.2826e-08],\n",
      "         [1.3042e-08, 1.2633e-09, 9.5772e-09,  ..., 7.1169e-08,\n",
      "          6.3678e-08, 6.5254e-08],\n",
      "         ...,\n",
      "         [1.5547e-11, 1.2162e-12, 6.0491e-12,  ..., 3.7400e-07,\n",
      "          3.8745e-07, 3.6814e-07],\n",
      "         [1.5809e-04, 4.5176e-03, 1.4398e-01,  ..., 2.5235e-18,\n",
      "          2.3198e-18, 2.2806e-18],\n",
      "         [7.4860e-08, 7.8633e-09, 6.7421e-08,  ..., 3.6045e-04,\n",
      "          3.3933e-04, 3.4390e-04]],\n",
      "\n",
      "        [[6.6588e-01, 3.3021e-01, 3.7524e-03,  ..., 6.7178e-09,\n",
      "          6.6396e-08, 5.0223e-08],\n",
      "         [3.3794e-11, 2.0617e-11, 4.5373e-11,  ..., 1.2056e-11,\n",
      "          4.4323e-07, 1.0843e-07],\n",
      "         [9.1014e-09, 8.8154e-10, 6.6832e-09,  ..., 1.3562e-01,\n",
      "          8.4361e-02, 8.0838e-02],\n",
      "         ...,\n",
      "         [1.1673e-11, 9.1314e-13, 4.5418e-12,  ..., 3.9658e-05,\n",
      "          7.3503e-02, 1.5065e-01],\n",
      "         [1.5809e-04, 4.5176e-03, 1.4398e-01,  ..., 9.2137e-10,\n",
      "          3.3764e-09, 5.5871e-10],\n",
      "         [2.7412e-08, 2.8794e-09, 2.4688e-08,  ..., 6.8538e-02,\n",
      "          2.0367e-01, 3.1548e-01]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_last_idx = 100\n",
    "wts = get_activation_wts(attention_model,\n",
    "                         Variable(torch.from_numpy(x_test_pad[:test_last_idx]).type(torch.LongTensor)))\n",
    "print(wts.size())\n",
    "visualize_attention(wts, x_test_pad[:test_last_idx], word_to_id, filename='attention.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T08:04:59.621938Z",
     "start_time": "2019-08-20T08:04:59.610941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function keras_preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.0)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(attention_model, x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
